import numpy as np
import gym

from gym import spaces
from gym.utils import seeding
import random

import math

class Navigation2DEnv(gym.Env):
    """2D navigation problems, as described in [1]. The code is adapted from 
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/maml_examples/point_env_randgoal.py

    At each time step, the 2D agent takes an action (its velocity, clipped in
    [-0.1, 0.1]), and receives a penalty equal to its L2 distance to the goal 
    position (ie. the reward is `-distance`). The 2D navigation tasks are 
    generated by sampling goal positions from the uniform distribution 
    on [-0.5, 0.5]^2.

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic 
        Meta-Learning for Fast Adaptation of Deep Networks", 2017 
        (https://arxiv.org/abs/1703.03400)
    """
    def __init__(self, task={}, low=-1.5, high=1.5):
        super(Navigation2DEnv, self).__init__()
        self.low = low
        self.high = high

        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,
            shape=(2,), dtype=np.float32)
        self.action_space = spaces.Box(low=-0.1, high=0.1,
            shape=(2,), dtype=np.float32)

        self._task = task
        self._goal = task.get('goal', np.zeros(2, dtype=np.float32))

        self._opening = task.get('opening', random.random() * 2)
        self.wall_epsilon = 0.1
        self.wall_radius = 0.7
        self.wall_upper_bound_rad = (self.wall_radius + self.wall_epsilon)**2
        self.wall_lower_bound_rad = (self.wall_radius - self.wall_epsilon)**2
        self.hole_epsilon = 0.1

        self._state = np.zeros(2, dtype=np.float32)
        self.seed()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def sample_tasks(self, num_tasks):
        goal_loc = []
        # wall_radius = math.sqrt(self.wall_radius_sq)
        # min_dist = `(wall_radius + self.wall_epsilon)**2 + 0.1
        while len(goal_loc) < num_tasks:
            x = random.random() * (self.high - self.low) - self.high
            y = random.random() * (self.high - self.low) - self.high
            if x**2+y**2 > self.wall_upper_bound_rad:
                goal_loc.append(np.array([x, y]))

        # goals = self.np_random.uniform(self.low, self.high, size=(num_tasks, 2))
        tasks = [{'goal': goal, 'opening': random.random() * 2} for goal in goal_loc]
        return tasks

    def reset_task(self, task):
        self._task = task
        self._goal = task['goal']

    def reset(self, env=True):
        self._state = np.zeros(2, dtype=np.float32)
        return self._state

    def compute_angle(self):
        x = self._state[0]
        y = self._state[1]
        theta = None
        if x == 0:
            if y > x:
                theta = math.pi/2
            else:
                theta = -math.pi/2
        else:
            modifier = 0
            if x < 0:
                modifier = math.pi
            theta = np.arctan(y/x) + modifier
        
        return theta

    def radius_squared(self):
        return np.sum(self._state**2)

    def step(self, action):
        action = np.clip(action, -0.1, 0.1)
        assert self.action_space.contains(action)
        self._state = self._state + action

        current_radius = self.radius_squared()
        if self.wall_lower_bound_rad < current_radius < self.wall_upper_bound_rad:
            if not (self._opening - self.hole_epsilon < self.compute_angle() < self._opening + self.hole_epsilon):
                self._state -= action
        
        x = self._state[0] - self._goal[0]
        y = self._state[1] - self._goal[1]
        reward = -np.sqrt(x ** 2 + y ** 2)
        done = ((np.abs(x) < 0.01) and (np.abs(y) < 0.01))

        return self._state, reward, done, {'task': self._task}
